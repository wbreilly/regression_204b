# type 3 SS w/in subs ANOVA
learna1 = aov_ez("subject", "rt", d25, within = c("intact","repetition", "block"))
learna1
# so just make new df with only those levels
learnd3 = d25 %>% filter(!repetition == 1 )
learnd3.ez = aov_ez("subject", "rt", learnd3, within = c("intact","repetition", "block"))
learnd3.ez
####################################################################################################
# break it down by block
d.block1 = d25 %>% filter(block == "study1")
d.block1.sumstats.rep.con = d.block1 %>% group_by(repetition,intact) %>% summarise(mean = mean(rt), SD = sd(rt))
# trial counts
d.block1.counts = count(d.block1, intact)
# repetition by condition and only pos's 2-5
# get n
d.block1.counts2 = count(group_by(d.block1,intact, repetition))
# add n to sumstats
d.block1.sumstats.rep.con[,5] = d.block1.counts2[,3]
# add SE
d.block1.sumstats.rep.con = mutate(d.block1.sumstats.rep.con, SE = SD/sqrt(n))
# create a bar graph
limits <- aes(ymax = d.block1.sumstats.rep.con$mean + d.block1.sumstats.rep.con$SE,
ymin = d.block1.sumstats.rep.con$mean - d.block1.sumstats.rep.con$SE)
b1.rep.con <- ggplot(data = d.block1.sumstats.rep.con, aes(x = factor(intact), y = mean,
fill = factor(repetition)))
b1.rep.con = b1.rep.con + geom_bar(stat = "identity",position = position_dodge(0.9)) +
geom_errorbar(limits, position = position_dodge(0.9),
width = 0.15) +
labs(x = "Condition", y = "Pos 2-5 Mean RT") +
ggtitle("Block 1 Learning pos 2-5 Mean RT by Condition and Repetition") +
scale_x_discrete("Conditions", labels = c("1" = "Intact", "2" = "Scrambled" ))
b1.rep.con = b1.rep.con + coord_cartesian(ylim=c(400,800))
b1.rep.con
###############
# block 2
# break it down by block
d.block2 = d25 %>% filter(block == "study2")
d.block2.sumstats.rep.con = d.block2 %>% group_by(repetition,intact) %>% summarise(mean = mean(rt), SD = sd(rt))
# trial counts
d.block2.counts = count(d.block2, intact)
# repetition by condition and only pos's 2-5
# get n
d.block2.counts2 = count(group_by(d.block2, intact, repetition))
# add n to sumstats
d.block2.sumstats.rep.con[,5] = d.block2.counts2[,3]
# add SE
d.block2.sumstats.rep.con = mutate(d.block2.sumstats.rep.con, SE = SD/sqrt(n))
# create a bar graph
limits <- aes(ymax = d.block2.sumstats.rep.con$mean + d.block2.sumstats.rep.con$SE,
ymin = d.block2.sumstats.rep.con$mean - d.block2.sumstats.rep.con$SE)
b2.rep.con <- ggplot(data = d.block2.sumstats.rep.con, aes(x = factor(intact), y = mean,
fill = factor(repetition)))
b2.rep.con = b2.rep.con + geom_bar(stat = "identity",position = position_dodge(0.9)) +
geom_errorbar(limits, position = position_dodge(0.9),
width = 0.15) +
labs(x = "Condition", y = "Pos 2-5 Mean RT") +
ggtitle("Block 2 Learning Mean RT by Condition and Repetition") +
scale_x_discrete("Conditions", labels = c("1" = "Intact", "2" = "Scrambled" ))
b2.rep.con = b2.rep.con + coord_cartesian(ylim=c(400,800))
b2.rep.con
library(tidyverse)
d = read.csv("~/walter/regression_204b/nonlinear.csv")
View(d)
p1 = plot(x1,y,d)
p1 = plot(x1,y1,d)
p1 = plot(x1,y1,data = d)
??plot
p1 = plot(d$x1,d$y1)
p2 = plot(d$x1,d$y2)
p3 = plot(d$x1,d$y3)
p1
p1 = plot(d$x1,d$y1)
plot(d$x1,d$y1)
m1 = lm(y1 ~ x1^2)
m1 = lm(y1 ~ x1^2, data = d)
m1
summary(m1)
d = read.csv("~/walter/regression_204b/nonlinearhw.csv")
d = read.csv("~/walter/regression_204b/nonlinearhw.csv")
plot(d$x1,d$y1)
plot(d$x1,d$y2)
plot(d$x1,d$y3)
plot(d$x1,d$y1)
m1 = lm(y1 ~ -log(x1), data = d)
summary(m1)
summary(m1)
abline(lm(formula = y1 ~ -log(x1), col = "red")
)
abline(lm(formula = d$y1 ~ -log(d$x1), col = "red")
)
abline(lm(formula = d$y1 ~ -log(d$x1)), col = "red")
plot(d$x1,d$y1)
abline(lm(formula = d$y1 ~ -log(d$x1)), col = "red")
abline(lm(formula = y1 ~ -log(x1), data = d), col = "red")
abline(lm(formula = y1 ~ -log(x1), data = d), col = "red")
abline(lm(formula = y1 ~ -log(x1), data = d)), col = "red")
abline(lm(formula = y1 ~ -log(x1), data = d), col = "red"))
abline(lm(formula = y1 ~ log(x1), data = d), col = "red")
abline(lm(formula = y1 ~ (x1), data = d), col = "red")
abline(lm(formula = y1 ~ x1^3, data = d), col = "red")
abline(lm(formula = y1 ~ -x1^3, data = d), col = "red")
abline(lm(formula = y1 ~ log(x1), data = d), col = "red")
abline(lm(formula = y1 ~ -log(x1), data = d), col = "red")
abline(lm(formula = y1 ~ nl(x1), data = d), col = "red")
abline(lm(formula = y1 ~ -exp(x1), data = d), col = "red")
d = read.csv("~/walter/regression_204b/nonlinearhw.csv")
plot(d$x1,d$y1)
abline(lm(formula = y1 ~ -exp(x1), data = d), col = "red")
c = coef(m1)
curve(c[1] + exp(c[2]*x), add = TRUE, lty = 1, col = "red")
plot(d$x1,d$y1)
m1 = lm(y1 ~ -log(x1), data = d)
summary(m1)
c = coef(m1)
curve(c[1] + exp(c[2]*x), add = TRUE, lty = 1, col = "red")
curve(c[1] + exp(c[2]*x1), add = TRUE, lty = 1, col = "red")
curve(c[1] + exp(c[2]*x), add = TRUE, lty = 1, col = "red")
curve(c[1] + -log(c[2]*x1), add = TRUE, lty = 1, col = "red")
curve(c[1] + -log(c[2]*d$x1), add = TRUE, lty = 1, col = "red")
curve(c[1] + -log(c[2]*x add = TRUE, lty = 1, col = "red")
curve(c[1] + -log(c[2]*x), add = TRUE, lty = 1, col = "red")
plot(d$x1,d$y1)
curve(c[1] + -log(c[2]*x), add = TRUE, lty = 1, col = "red")
m1 = lm(y1 ~ -log(x1), data = d)
summary(m1)
c = coef(m1)
curve(c[1] + -log(c[2]*x), add = TRUE, lty = 1, col = "red")
m1 = lm(y1 ~ -(x1)^3, data = d)
summary(m1)
c = coef(m1)
curve(c[1] + -log(c[2]*x), add = TRUE, lty = 1, col = "red")
setwd("~/walter/regression_204b")
d <- read.csv("interaction01.csv")
summary(d)
d$xf <- d$x*d$female
d$zy <- scale(d$y)[,1]
d$zx <- scale(d$x)[,1]
d$zfemale <- scale(d$female)[,1]
View(d)
d$zxf <- d$zx*d$zfemale
cor(d[,c('x','female','xf')])
cor(d[,c('zx','zfemale','zxf')])
m1 <- lm(y ~ x*female, data = d)
m2 <- lm(zy ~ zx*zfemale, data = d)
summary(m1)
summary(m2)
df <- d[which(d$female == 1),]
dm <- d[which(d$female == 0),]
m1f <- lm(y ~ x, data = df)
m1m <- lm(y ~ x, data = dm)
summary(m1f)
summary(m1m)
par(mfrow = c(1,3))
plot(d$x, d$y)
abline(lm(d$y ~ d$x), col = "red")
plot(dm$x, dm$y)
abline(lm(y ~ x, data = dm), col = "red")
plot(df$x, df$y)
abline(lm(y ~ x, data = df), col = "red")
#or
par(mfrow = c(1,1))
plot(d$x, d$y)
points(df$x, df$y, col = 'red')
points(dm$x, dm$y, col = 'Green')
abline(lm(y~x, data = df), col = 'red')
abline(lm(y~x, data = dm), col = 'green')
d <- read.csv("lab06dat1.csv")
d <- read.csv("lab06dat1.csv")
d <- read.csv("lab06dat.csv")
head(d)
summary(d)
table(d$serviceanimal)
m1 <- lm(perceivedfree ~ serviceanimal, data = d)
m2 <- lm(independence ~ serviceanimal, data = d)
m3 <- lm(happy ~ serviceanimal, data = d)
anova(m1)
anova(m2)
anova(m3)
summary(m1)
by(d$perceivedfree, d$serviceanimal, mean)-
by(d$perceivedfree, d$serviceanimal, mean)[[1]]
summary(m2)
by(d$independence, d$serviceanimal, mean)-
by(d$independence, d$serviceanimal, mean)[[1]]
summary(m3)
by(d$happy, d$serviceanimal, mean)-
by(d$happy, d$serviceanimal, mean)[[1]]
summary(m3)
t.test(d[d$dog == 1,"happy"],d[d$cat == 1,"happy"],var.equal = TRUE)
t.test(d[d$other == 1,"happy"],d[d$cat == 1,"happy"],var.equal = TRUE)
m4 <- lm(happy ~ perceivedfree + independence +
perceivedfree*independence, data = d)
summary(m4)
#How will you think about the data? Is perceived freedom moderating
#We'll go with perceived freedom as the moderator.
pf <- seq(min(d$perceivedfree),max(d$perceivedfree),.1)
bi <- coef(m4)[3] + pf*coef(m4)[4]
bi
plot(pf,bi,main = "Moderation of Independence by Perceived Freedom",
ylab = "Association of Independence and Happiness",
xlab = "Perceived Freedom Score")
points(
mean(d$perceivedfree),
(coef(m4)[3]+coef(m4)[4]*mean(d$perceivedfree)),
cex = 3, pch = 16, col = "red")
#or
n2sd <- -1*sd(d$perceivedfree)+mean(d$perceivedfree)
p2sd <- 1*sd(d$perceivedfree)+mean(d$perceivedfree)
csd <- mean(d$perceivedfree)
n2sd;csd;p2sd
coef(m4)
d$hn2 <- coef(m4)[1] + coef(m4)[2]*n2sd +
coef(m4)[3]*d$independence + coef(m4)[4]*n2sd*d$independence
d$hcsd <- coef(m4)[1] + coef(m4)[2]*csd +
coef(m4)[3]*d$independence + coef(m4)[4]*csd*d$independence
d$hp2 <- coef(m4)[1] + coef(m4)[2]*p2sd +
coef(m4)[3]*d$independence + coef(m4)[4]*p2sd*d$independence
d <- d[order(d$independence),]
plot(d$independence,d$happy,
type = "n",
main = "Moderated Associations of Independence and Happiness",
ylab = "Association of Independence and Happiness",
xlab = "Independence Scores")
lines(d$independence,d$hn2, lwd = 3, col = "blue")
lines(d$independence,d$hcsd, lwd = 3, col = "red")
lines(d$independence,d$hp2, lwd = 3, col = "green")
lines(c(0,0),c(0,10))
legend(-2,38,lty = c(1,1,1),col = c("blue","red","green"),
legend = c("-1SD","0SD","1SD"))
d <- read.csv("interaction01.csv")
summary(d)
View(d)
d$xf <- d$x*d$female
d$zy <- scale(d$y)[,1]
d$zx <- scale(d$x)[,1]
d$zfemale <- scale(d$female)[,1]
d$zxf <- d$zx*d$zfemale
cor(d[,c('x','female','xf')])
cor(d[,c('zx','zfemale','zxf')])
m1 <- lm(y ~ x*female, data = d)
m2 <- lm(zy ~ zx*zfemale, data = d)
summary(m1)
summary(m2)
d <- read.csv("interaction01.csv")
summary(d)
#generate an interaction term for female*x
d$xf <- d$x*d$female
#z-norm all predictors
d$zy <- scale(d$y)[,1]
d$zx <- scale(d$x)[,1]
d$zfemale <- scale(d$female)[,1]
#generate 3-way interaction term
d$zxf <- d$zx*d$zfemale
#Correlations between predictors and their interaction term
#Are the correlations going to change after the z-transformation?
cor(d[,c('x','female','xf')])
cor(d[,c('zx','zfemale','zxf')])
#linear transformations do not change the relationships b/w the variables
m1 <- lm(y ~ x*female, data = d)
m2 <- lm(zy ~ zx*zfemale, data = d)
summary(m1)
summary(m2)
m1
summary(m1)
d = read.csv("cinteraction.csv")
d = read.csv("cinteraction.csv")
plot(d$fluidint,d$relevantknow)
library(afex)
m1 = summary(fluidint ~ time*relevantknow, data =d)
summary(m1())
summary(m1)
m1 = summary(fluidint ~ time*relevantknow, data =d)
summary(m1)
m1 = summary(d$fluidint ~ d$time*d$relevantknow)
summary(m1)
class(d$fluidint)
class(d$relevantknow)
class(d$time)
m1 = lm(d$fluidint ~ d$time*d$relevantknow)
summary(m1)
m1 = lm(d$time ~ d$fluidint*d$relevantknow)
summary(m1)
d <- read.csv("lab06dat.csv")
head(d)
summary(d)
#Dummy coding in regression -----------------------------------------------------------
table(d$serviceanimal)
m1 <- lm(perceivedfree ~ serviceanimal, data = d)
m2 <- lm(independence ~ serviceanimal, data = d)
m3 <- lm(happy ~ serviceanimal, data = d)
anova(m1)
anova(m2)
anova(m3)
summary(m1)
by(d$perceivedfree, d$serviceanimal, mean)-
by(d$perceivedfree, d$serviceanimal, mean)[[1]]
summary(m2)
by(d$independence, d$serviceanimal, mean)-
by(d$independence, d$serviceanimal, mean)[[1]]
summary(m3)
by(d$happy, d$serviceanimal, mean)-
by(d$happy, d$serviceanimal, mean)[[1]]
summary(m3)
t.test(d[d$dog == 1,"happy"],d[d$cat == 1,"happy"],var.equal = TRUE)
t.test(d[d$other == 1,"happy"],d[d$cat == 1,"happy"],var.equal = TRUE)
#differences in SE, t, and p due to differences in df.
#Interactions w/ continuous variables ----------------------------------------------------------
m4 <- lm(happy ~ perceivedfree + independence +
perceivedfree*independence, data = d)
summary(m4)
#How will you think about the data? Is perceived freedom moderating
#the relation of independence and happiness, or is independence
#moderating the relation of perceived freedom and happiness?
#We'll go with perceived freedom as the moderator.
pf <- seq(min(d$perceivedfree),max(d$perceivedfree),.1)
bi <- coef(m4)[3] + pf*coef(m4)[4]
bi
plot(pf,bi,main = "Moderation of Independence by Perceived Freedom",
ylab = "Association of Independence and Happiness",
xlab = "Perceived Freedom Score")
points(
mean(d$perceivedfree),
(coef(m4)[3]+coef(m4)[4]*mean(d$perceivedfree)),
cex = 3, pch = 16, col = "red")
#or
n2sd <- -1*sd(d$perceivedfree)+mean(d$perceivedfree)
p2sd <- 1*sd(d$perceivedfree)+mean(d$perceivedfree)
csd <- mean(d$perceivedfree)
n2sd;csd;p2sd
coef(m4)
d$hn2 <- coef(m4)[1] + coef(m4)[2]*n2sd +
coef(m4)[3]*d$independence + coef(m4)[4]*n2sd*d$independence
d$hcsd <- coef(m4)[1] + coef(m4)[2]*csd +
coef(m4)[3]*d$independence + coef(m4)[4]*csd*d$independence
d$hp2 <- coef(m4)[1] + coef(m4)[2]*p2sd +
coef(m4)[3]*d$independence + coef(m4)[4]*p2sd*d$independence
d <- d[order(d$independence),]
plot(d$independence,d$happy,
type = "n",
main = "Moderated Associations of Independence and Happiness",
ylab = "Association of Independence and Happiness",
xlab = "Independence Scores")
lines(d$independence,d$hn2, lwd = 3, col = "blue")
lines(d$independence,d$hcsd, lwd = 3, col = "red")
lines(d$independence,d$hp2, lwd = 3, col = "green")
lines(c(0,0),c(0,10))
legend(-2,38,lty = c(1,1,1),col = c("blue","red","green"),
legend = c("-1SD","0SD","1SD"))
summary(m4)
plot(pf,bi,main = "Moderation of Independence by Perceived Freedom",
ylab = "Association of Independence and Happiness",
xlab = "Perceived Freedom Score")
points(
mean(d$perceivedfree),
(coef(m4)[3]+coef(m4)[4]*mean(d$perceivedfree)),
cex = 3, pch = 16, col = "red")
n2sd <- -1*sd(d$perceivedfree)+mean(d$perceivedfree)
p2sd <- 1*sd(d$perceivedfree)+mean(d$perceivedfree)
csd <- mean(d$perceivedfree)
n2sd;csd;p2sd
plot(d$independence,d$happy,
type = "n",
main = "Moderated Associations of Independence and Happiness",
ylab = "Association of Independence and Happiness",
xlab = "Independence Scores")
lines(d$independence,d$hn2, lwd = 3, col = "blue")
lines(d$independence,d$hcsd, lwd = 3, col = "red")
lines(d$independence,d$hp2, lwd = 3, col = "green")
lines(c(0,0),c(0,10))
legend(-2,38,lty = c(1,1,1),col = c("blue","red","green"),
legend = c("-1SD","0SD","1SD"))
??aov_car
m1 = aov_car(d$time ~ d$fluidint*d$relevantknow)
library(afex)
m1 = aov_car(d$time ~ d$fluidint*d$relevantknow)
m1 = aov_car(time ~ fluidint*relevantknow, data =d)
View(d)
# 3.2.17
library(afex)
d = read.csv("cinteraction.csv")
plot(d$fluidint,d$relevantknow)
m1 = aov_car(time ~ fluidint*relevantknow, data =d)
m1 = aov_car(d$time ~ d$fluidint*d$relevantknow, data =d)
summary(m1)
m1 = aov_car(d$time ~ d$fluidint*d$relevantknow + Error(id), data =d)
summary(m1)
m1 = aov_car(d$time ~ d$fluidint*d$relevantknow + Error, data =d)
summary(m1)
View(d)
m1 = aov_car(d$time ~ d$fluidint * d$relevantknow + Error, data =d)
summary(m1)
m1 = lm(d$time ~ d$fluidint * d$relevantknow, data =d)
summary(m1)
x1 <- d$fluidint
x2 <- d$relevantknow
y <- d$time
dat <- data.frame(y=y,x1=x1,x2=x2)
res <- lm(y~x1*x2,data=dat)
z1 <- z2 <- seq(-1,1)
newdf <- expand.grid(x1=z1,x2=z2)
library(ggplot2)
p <- ggplot(data=transform(newdf, yp=predict(res, newdf)),
aes(y=yp, x=x1, color=factor(x2))) + stat_smooth(method=lm)
p + scale_colour_discrete(name="x2") +
labs(x="x1", y="mean of resp") +
scale_x_continuous(breaks=seq(-1,1)) + theme_bw()
x1 <- d$relevantknow
x2 <- d$fluidint
y <- d$time
dat <- data.frame(y=y,x1=x1,x2=x2)
res <- lm(y~x1*x2,data=dat)
z1 <- z2 <- seq(-1,1)
newdf <- expand.grid(x1=z1,x2=z2)
library(ggplot2)
p <- ggplot(data=transform(newdf, yp=predict(res, newdf)),
aes(y=yp, x=x1, color=factor(x2))) + stat_smooth(method=lm)
p + scale_colour_discrete(name="x2") +
labs(x="x1", y="mean of resp") +
scale_x_continuous(breaks=seq(-1,1)) + theme_bw()
View(newdf)
View(dat)
View(newdf)
x1 <- scale(d$relevantknow)
x1 <- scale(d$relevantknow)
x2 <- scale(d$fluidint)
y <- scale(d$time)
dat <- data.frame(y=y,x1=x1,x2=x2)
res <- lm(y~x1*x2,data=dat)
z1 <- z2 <- seq(-1,1)
newdf <- expand.grid(x1=z1,x2=z2)
library(ggplot2)
p <- ggplot(data=transform(newdf, yp=predict(res, newdf)),
aes(y=yp, x=x1, color=factor(x2))) + stat_smooth(method=lm)
p + scale_colour_discrete(name="x2") +
labs(x="Relevant Knowledge", y="Time") +
scale_x_continuous(breaks=seq(-1,1)) + theme_bw()
x2 <- scale(d$relevantknow)
x1 <- scale(d$fluidint)
y <- scale(d$time)
dat <- data.frame(y=y,x1=x1,x2=x2)
res <- lm(y~x1*x2,data=dat)
z1 <- z2 <- seq(-1,1)
newdf <- expand.grid(x1=z1,x2=z2)
p <- ggplot(data=transform(newdf, yp=predict(res, newdf)),
aes(y=yp, x=x1, color=factor(x2))) + stat_smooth(method=lm)
p + scale_colour_discrete(name="x2") +
labs(x="Relevant Knowledge", y="Time") +
scale_x_continuous(breaks=seq(-1,1)) + theme_bw()
summary(m1)
x1 <- scale(d$relevantknow)
x2 <- scale(d$fluidint)
y <- scale(d$time)
dat <- data.frame(y=y,x1=x1,x2=x2)
res <- lm(y~x1*x2,data=dat)
z1 <- z2 <- seq(-1,1)
newdf <- expand.grid(x1=z1,x2=z2)
p <- ggplot(data=transform(newdf, yp=predict(res, newdf)),
aes(y=yp, x=x1, color=factor(x2))) + stat_smooth(method=lm)
p + scale_colour_discrete(name="x2") +
labs(x="Relevant Knowledge", y="Time") +
scale_x_continuous(breaks=seq(-1,1)) + theme_bw()
View(newdf)
p <- ggplot(data=transform(newdf, yp=predict(res, newdf)),
aes(y=yp, x=x1, color=factor(x2))) + stat_smooth(method=lm)
p + scale_colour_discrete(name="FluidInt") +
labs(x="Relevant Knowledge", y="Time") +
scale_x_continuous(breaks=seq(-1,1)) + theme_bw()
p <- ggplot(data=transform(newdf, yp=predict(res, newdf)),
aes(y=yp, x=x1, color=factor(x2))) + stat_smooth(method=lm)
p + scale_colour_discrete(name="FluidInt") +
labs(x="Relevant Knowledge", y="Time") +
ggtitle("Interaction of Relevant Knowledge and Fluid Intelligence")
scale_x_continuous(breaks=seq(-1,1)) + theme_bw()
summary(m1)
set.seed(1234)
b0 = 130
b1 = 2.5
n = 100
x = rnorm(n, 10, 0.785)
error = rnorm(100)
y = b0 + b1*x + error
plot(x, y, xlab = "Books Read", ylab = "GRE Verbal Score", main = "Scatterplot of Books Read vs. GRE Verbal Score")
mod1 = lm(y ~ x) #fit a model to the simulated data
abline(mod1, col = "red") #overlay the regression line
#Population parameters for University 1
b0_u1 = 130 #intercept
b1_u1 = 2.5 #slope
n = 100
x = rnorm(n, 10, 0.785) #number of books read as a normal random 					    variable
error = rnorm(100) #standard normal error term
y_u1 = b0_u1 + b1_u1*x + error #population generating model for 							   University 1
#Population parameters for University 2
b0_u2 = 140 #intercept, higher intercept than University 1...
b1_u2 = 1 #slope, ...but smaller slope than University 1
y_u2 = b0_u2 + b1_u2*x + error #population generating model for 							   University 2
#Population parameters for University 3
b0_u3 = 120 #intercept
b1_u3 = 3 #slope
y_u3 = b0_u3 + b1_u3*x + error
#population generating model for University 3
#Create a combined data frame for all 3 universities
data = data.frame(x, y_u1, y_u2, y_u3)
mod1 = lm(y_u1 ~ x, data = data) #University 1
mod2 = lm(y_u2 ~ x, data = data) #University 2
mod3 = lm(y_u3 ~ x, data = data) #University 3
#Now we can plot each university on the same plot
plot(x, y_u1, xlab = "Books Read", ylab = "GRE Verbal Score", main = "GRE Regressed on Books Read", col = "red", ylim = c(140, 160))
par(new = TRUE)
plot(x, y_u2, xlab = "Books Read", ylab = "GRE Verbal Score", main = "GRE Regressed on Books Read", col = "blue", ylim = c(140, 160))
par(new = TRUE)
plot(x, y_u3, xlab = "Books Read", ylab = "GRE Verbal Score", main = "GRE Regressed on Books Read", col = "dark green", ylim = c(140, 160))
legend(10.5, 145, c("University 1", "University 2", "University 3"), col = c("red", "blue", "dark green"), pch = c(16, 16))
